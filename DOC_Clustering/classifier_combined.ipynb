{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import permutation_test_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "from sklearn import metrics\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import dataimport.prepareDataset as prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(141)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "are there features which don't have any further content/ don't change over the whole periode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Part_chro=['13','22','10', '18']\n",
    "Part_reco=['19','20','02','09']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "[X_Base,X_Anes,X_Reco,Y_ID_Base,Y_ID_Anes,Y_ID_Reco,\n",
    " Y_out_Anes,Y_out_Base,Y_out_Reco ]=prep.prepare_Dataset('data/combined_NEW_wPLI_dPLI_all_10_1_left.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define 30% test set \n",
    "!!!! only used to select the hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#X_train,X_test,Y_train,Y_test=train_test_split(X,Y_out,Y_ID,random_state=0,test_size=0.3)\n",
    "X_train_Base,X_test_Base,Y_train_Base,Y_test_Base,Y_ID_Base_train,Y_ID_Base_test=train_test_split(X_Base,Y_out_Base,Y_ID_Base,random_state=0,test_size=0.3)\n",
    "X_train_Reco,X_test_Reco,Y_train_Reco,Y_test_Reco,Y_ID_Reco_train,Y_ID_Reco_test=train_test_split(X_Reco,Y_out_Reco,Y_ID_Reco,random_state=0,test_size=0.3)\n",
    "X_train_Anes,X_test_Anes,Y_train_Anes,Y_test_Anes,Y_ID_Anes_train,Y_ID_Anes_test=train_test_split(X_Anes,Y_out_Anes,Y_ID_Anes,random_state=0,test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr=LogisticRegression(random_state=0,penalty='l1',C=4,solver='liblinear',max_iter=10000)\n",
    "\n",
    "#score_B, permutation_scores_B, pvalue_B = permutation_test_score(\n",
    "#    lr, X_Base,Y_out_Base, scoring=\"accuracy\", n_permutations=100, n_jobs=-1,cv=3)\n",
    "\n",
    "#score_A, permutation_scores_A, pvalue_A = permutation_test_score(\n",
    "#    lr, X_Anes,Y_out_Anes, scoring=\"accuracy\", n_permutations=100, n_jobs=-1,cv=3)\n",
    "\n",
    "#score_R, permutation_scores_R, pvalue_R = permutation_test_score(\n",
    "#    lr, X_Reco,Y_out_Reco, scoring=\"accuracy\", n_permutations=100, n_jobs=-1,cv=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nplt.figure(figsize= [15, 6])\\nplt.subplot(131)\\nplt.hist(permutation_scores_B, 20, label='Permutation scores Baseline')\\nylim = plt.ylim()\\nplt.plot(2 * [score_B], ylim, '--g', linewidth=3,\\n         label='Classification Score'\\n         ' (pvalue %s)' % pvalue_B)\\nplt.plot(2 * [1. / 2], ylim, '--k', linewidth=3, label='Luck')\\nplt.legend()\\nplt.xlabel('Score')\\n\\nplt.subplot(132)\\nplt.hist(permutation_scores_A, 20, label='Permutation scores Anesthesia')\\nylim = plt.ylim()\\nplt.plot(2 * [score_A], ylim, '--g', linewidth=3,\\n         label='Classification Score'\\n         ' (pvalue %s)' % pvalue_A)\\nplt.plot(2 * [1. / 2], ylim, '--k', linewidth=3, label='Luck')\\nplt.legend()\\nplt.xlabel('Score')\\n\\nplt.subplot(133)\\nplt.hist(permutation_scores_R, 20, label='Permutation scores Recovery')\\nylim = plt.ylim()\\nplt.plot(2 * [score_R], ylim, '--g', linewidth=3,\\n         label='Classification Score'\\n         ' (pvalue %s)' % pvalue_R)\\nplt.plot(2 * [1. / 2], ylim, '--k', linewidth=3, label='Luck')\\n\\nplt.legend()\\nplt.xlabel('Score')\\nplt.savefig('combined_LogReg_permutation.png',dpi=150)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View histogram of permutation scores\n",
    "\"\"\"\n",
    "plt.figure(figsize= [15, 6])\n",
    "plt.subplot(131)\n",
    "plt.hist(permutation_scores_B, 20, label='Permutation scores Baseline')\n",
    "ylim = plt.ylim()\n",
    "plt.plot(2 * [score_B], ylim, '--g', linewidth=3,\n",
    "         label='Classification Score'\n",
    "         ' (pvalue %s)' % pvalue_B)\n",
    "plt.plot(2 * [1. / 2], ylim, '--k', linewidth=3, label='Luck')\n",
    "plt.legend()\n",
    "plt.xlabel('Score')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.hist(permutation_scores_A, 20, label='Permutation scores Anesthesia')\n",
    "ylim = plt.ylim()\n",
    "plt.plot(2 * [score_A], ylim, '--g', linewidth=3,\n",
    "         label='Classification Score'\n",
    "         ' (pvalue %s)' % pvalue_A)\n",
    "plt.plot(2 * [1. / 2], ylim, '--k', linewidth=3, label='Luck')\n",
    "plt.legend()\n",
    "plt.xlabel('Score')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.hist(permutation_scores_R, 20, label='Permutation scores Recovery')\n",
    "ylim = plt.ylim()\n",
    "plt.plot(2 * [score_R], ylim, '--g', linewidth=3,\n",
    "         label='Classification Score'\n",
    "         ' (pvalue %s)' % pvalue_R)\n",
    "plt.plot(2 * [1. / 2], ylim, '--k', linewidth=3, label='Luck')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Score')\n",
    "plt.savefig('combined_LogReg_permutation.png',dpi=150)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata = [[score_B, pvalue_B], \\n        [score_A, pvalue_A],\\n        [score_R, pvalue_R]]\\n\\npd.DataFrame(data, columns=[\"permutation LR\", \"pvalue\"],index=[\\'Baseline\\',\\'Anesthesia\\',\\'Recovery\\'])\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "data = [[score_B, pvalue_B], \n",
    "        [score_A, pvalue_A],\n",
    "        [score_R, pvalue_R]]\n",
    "\n",
    "pd.DataFrame(data, columns=[\"permutation LR\", \"pvalue\"],index=['Baseline','Anesthesia','Recovery'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "LOGISTIC REGRESSION\n",
    "  I Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cs=np.arange(1,4,0.5)\n",
    "lr_accuracy_Base=[]\n",
    "lr_accuracy_Anes=[]\n",
    "lr_accuracy_Reco=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1b80edb26a65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_Base\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train_Base\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mP_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_Base\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlr_accuracy_Base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test_Base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1532\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1533\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    720\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 542\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "for c in cs:\n",
    "    lr=LogisticRegression(random_state=0,penalty='l1',C=c,solver='liblinear',max_iter=10000)\n",
    "    lr.fit(X_train_Base,Y_train_Base)\n",
    "    P_lr=lr.predict(X_test_Base)\n",
    "    lr_accuracy_Base.append(metrics.accuracy_score(Y_test_Base, P_lr))\n",
    "\n",
    "    lr=LogisticRegression(random_state=0,penalty='l1',C=c,solver='liblinear',max_iter=10000)\n",
    "    lr.fit(X_train_Anes,Y_train_Anes)\n",
    "    P_lr=lr.predict(X_test_Anes)\n",
    "    lr_accuracy_Anes.append(metrics.accuracy_score(Y_test_Anes, P_lr))\n",
    "\n",
    "    lr=LogisticRegression(random_state=0,penalty='l1',C=c,solver='liblinear',max_iter=10000)\n",
    "    lr.fit(X_train_Reco,Y_train_Reco)\n",
    "    P_lr=lr.predict(X_test_Reco)\n",
    "    lr_accuracy_Reco.append(metrics.accuracy_score(Y_test_Reco, P_lr))\n",
    "\n",
    "#feat_importances_Reco = pd.Series(lr.coef_[0], index=X.columns)\n",
    "#feat_importances_Reco.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(cs,lr_accuracy_Base)\n",
    "plt.plot(cs,lr_accuracy_Anes)\n",
    "plt.plot(cs,lr_accuracy_Reco)\n",
    "plt.ylabel('Accuracy [%]')\n",
    "plt.xlabel('C')\n",
    "plt.title('Linear Regression')\n",
    "plt.legend(['Baseline','Anesthesia','Recovery'])\n",
    "plt.savefig('combined_LogReg_hyper.png',dpi=150)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr=LogisticRegression(random_state=0,penalty='l1',C=c,solver='liblinear',max_iter=10000)\n",
    "lr.fit(X_train_Anes,Y_train_Anes)\n",
    "P_lr=lr.predict(X_test_Anes)\n",
    "lr_accuracy_Anes.append(metrics.accuracy_score(Y_test_Anes, P_lr))\n",
    "\n",
    "lr=LogisticRegression(random_state=0,penalty='l1',C=c,solver='liblinear',max_iter=10000)\n",
    "lr.fit(X_train_Reco,Y_train_Reco)\n",
    "P_lr=lr.predict(X_test_Reco)\n",
    "lr_accuracy_Reco.append(metrics.accuracy_score(Y_test_Reco, P_lr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "cv_LR_Base=[]\n",
    "cv_LR_Anes=[]\n",
    "cv_LR_Reco=[]\n",
    "\n",
    "FI_LR_Base=[]\n",
    "FI_LR_Anes=[]\n",
    "FI_LR_Reco=[]\n",
    "\n",
    "for r in range(0,4):\n",
    "    for c in range (0,4):\n",
    "        tmp_X_test_Base=X_Base[(Y_ID_Base == Part_reco[r]) | (Y_ID_Base == Part_chro[c])]\n",
    "        tmp_X_train_Base=X_Base[(Y_ID_Base != Part_reco[r]) & (Y_ID_Base != Part_chro[c])]\n",
    "        tmp_Y_test_Base=Y_out_Base[(Y_ID_Base == Part_reco[r]) | (Y_ID_Base == Part_chro[c])]\n",
    "        tmp_Y_train_Base=Y_out_Base[(Y_ID_Base != Part_reco[r]) & (Y_ID_Base != Part_chro[c])]\n",
    "        \n",
    "        tmp_X_test_Anes=X_Anes[(Y_ID_Anes == Part_reco[r]) | (Y_ID_Anes == Part_chro[c])]\n",
    "        tmp_X_train_Anes=X_Anes[(Y_ID_Anes != Part_reco[r]) & (Y_ID_Anes != Part_chro[c])]\n",
    "        tmp_Y_test_Anes=Y_out_Anes[(Y_ID_Anes == Part_reco[r]) | (Y_ID_Anes == Part_chro[c])]\n",
    "        tmp_Y_train_Anes=Y_out_Anes[(Y_ID_Anes != Part_reco[r]) & (Y_ID_Anes != Part_chro[c])]\n",
    "\n",
    "        tmp_X_test_Reco=X_Reco[(Y_ID_Reco == Part_reco[r]) | (Y_ID_Reco == Part_chro[c])]\n",
    "        tmp_X_train_Reco=X_Reco[(Y_ID_Reco != Part_reco[r]) & (Y_ID_Reco != Part_chro[c])]\n",
    "        tmp_Y_test_Reco=Y_out_Reco[(Y_ID_Reco == Part_reco[r]) | (Y_ID_Reco == Part_chro[c])]\n",
    "        tmp_Y_train_Reco=Y_out_Reco[(Y_ID_Reco != Part_reco[r]) & (Y_ID_Reco != Part_chro[c])]\n",
    "\n",
    "        lr = LogisticRegression(random_state=0, penalty='l1', C=4,max_iter=1000,solver='liblinear')\n",
    "        lr.fit(tmp_X_train_Base, tmp_Y_train_Base)\n",
    "        P_lr = lr.predict(tmp_X_test_Base)\n",
    "        cv_LR_Base.append(metrics.accuracy_score(tmp_Y_test_Base, P_lr))\n",
    "        FI_LR_Base.append(lr.coef_)\n",
    "        \n",
    "\n",
    "        lr = LogisticRegression(random_state=0, penalty='l1', C=4,max_iter=1000,solver='liblinear')\n",
    "        lr.fit(tmp_X_train_Anes, tmp_Y_train_Anes)\n",
    "        P_lr = lr.predict(tmp_X_test_Anes)\n",
    "        cv_LR_Anes.append(metrics.accuracy_score(tmp_Y_test_Anes, P_lr))\n",
    "        FI_LR_Anes.append(lr.coef_)\n",
    "        \n",
    "        lr = LogisticRegression(random_state=0, penalty='l1', C=4,max_iter=1000,solver='liblinear')\n",
    "        lr.fit(tmp_X_train_Reco, tmp_Y_train_Reco)\n",
    "        P_lr = lr.predict(tmp_X_test_Reco)\n",
    "        cv_LR_Reco.append(metrics.accuracy_score(tmp_Y_test_Reco, P_lr))\n",
    "        FI_LR_Reco.append(lr.coef_)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(cv_LR_Base)\n",
    "plt.plot(cv_LR_Anes)\n",
    "plt.plot(cv_LR_Reco)\n",
    "plt.legend(['Baseline','Anesthesia','Recovery'])\n",
    "plt.xlabel('cross validation')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('combined_CV')\n",
    "plt.savefig('combined_LogRefg_CV.png',dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "right_Anes = np.where(np.array(cv_LR_Anes) > 0.5)[0]\n",
    "right_Base = np.where(np.array(cv_LR_Base) > 0.5)[0]\n",
    "right_Reco = np.where(np.array(cv_LR_Reco) > 0.5)[0]\n",
    "\n",
    "FI_LR_Base= list(FI_LR_Base[i] for i in right_Base)\n",
    "FI_LR_Anes= list(FI_LR_Anes[i] for i in right_Anes)\n",
    "FI_LR_Reco= list(FI_LR_Reco[i] for i in right_Reco)\n",
    "\n",
    "feat_importances_Base_LR = pd.Series(abs(np.mean(FI_LR_Base[0:],axis=0)[0]), index=X_Base.columns)\n",
    "feat_importances_Anes_LR = pd.Series(abs(np.mean(FI_LR_Anes[0:],axis=0)[0]), index=X_Base.columns)\n",
    "feat_importances_Reco_LR = pd.Series(abs(np.mean(FI_LR_Reco[0:],axis=0)[0]), index=X_Base.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= [10, 6])\n",
    "plt.subplot(121)\n",
    "feat_importances_Base_LR.plot(kind='barh')\n",
    "plt.title('LR_Baseline')\n",
    "plt.subplot(122)\n",
    "feat_importances_Anes_LR.plot(kind='barh',color='orange')\n",
    "plt.title('Anesthesia')\n",
    "plt.savefig('combined_LogRefg_features.png',dpi=150)\n",
    "#plt.subplot(133)\n",
    "#feat_importances_Reco_LR.plot(kind='barh',color='green')\n",
    "#plt.title('Recovery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(cv_LR_Base)\n",
    "np.std(cv_LR_Base)\n",
    "\n",
    "np.mean(cv_LR_Anes)\n",
    "np.std(cv_LR_Anes)\n",
    "\n",
    "np.mean(cv_LR_Reco)\n",
    "np.std(cv_LR_Reco)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "SVM (sklearn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cs=np.arange(0.3,6,0.2)\n",
    "\n",
    "svm_accuracy_Base=[]\n",
    "svm_accuracy_Anes=[]\n",
    "svm_accuracy_Reco=[]\n",
    "\n",
    "for c in cs:\n",
    "    svm_model = svm.LinearSVC(C=c, loss=\"hinge\" , max_iter=100000)\n",
    "    svm_model.fit(X_train_Base,Y_train_Base)\n",
    "    P_lr=svm_model.predict(X_test_Base)\n",
    "    svm_accuracy_Base.append(metrics.accuracy_score(Y_test_Base, P_lr))\n",
    "\n",
    "    svm_model = svm.LinearSVC(C=c, loss=\"hinge\", max_iter=100000)\n",
    "    svm_model.fit(X_train_Anes,Y_train_Anes)\n",
    "    P_lr=svm_model.predict(X_test_Anes)\n",
    "    svm_accuracy_Anes.append(metrics.accuracy_score(Y_test_Anes, P_lr))\n",
    "\n",
    "    svm_model = svm.LinearSVC(C=c, loss=\"hinge\", max_iter=100000)\n",
    "    svm_model.fit(X_train_Reco,Y_train_Reco)\n",
    "    P_lr=svm_model.predict(X_test_Reco)\n",
    "    svm_accuracy_Reco.append(metrics.accuracy_score(Y_test_Reco, P_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(cs,svm_accuracy_Base)\n",
    "plt.plot(cs,svm_accuracy_Anes)\n",
    "plt.plot(cs,svm_accuracy_Reco)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('C')\n",
    "plt.title('Support Vector Machine')\n",
    "plt.legend(['Baseline','Anesthesia','Recovery'])\n",
    "plt.show()\n",
    "plt.savefig('combined_SVM_hyper.png',dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cv_SVM_Base=[]\n",
    "cv_SVM_Anes=[]\n",
    "cv_SVM_Reco=[]\n",
    "\n",
    "FI_SVM_Base=[]\n",
    "FI_SVM_Anes=[]\n",
    "FI_SVM_Reco=[]\n",
    "\n",
    "\n",
    "for r in range(0,4):\n",
    "    for c in range (0,4):\n",
    "        tmp_X_test_Base=X_Base[(Y_ID_Base == Part_reco[r]) | (Y_ID_Base == Part_chro[c])]\n",
    "        tmp_X_train_Base=X_Base[(Y_ID_Base != Part_reco[r]) & (Y_ID_Base != Part_chro[c])]\n",
    "        tmp_Y_test_Base=Y_out_Base[(Y_ID_Base == Part_reco[r]) | (Y_ID_Base == Part_chro[c])]\n",
    "        tmp_Y_train_Base=Y_out_Base[(Y_ID_Base != Part_reco[r]) & (Y_ID_Base != Part_chro[c])]\n",
    "\n",
    "        tmp_X_test_Anes=X_Anes[(Y_ID_Anes == Part_reco[r]) | (Y_ID_Anes == Part_chro[c])]\n",
    "        tmp_X_train_Anes=X_Anes[(Y_ID_Anes != Part_reco[r]) & (Y_ID_Anes != Part_chro[c])]\n",
    "        tmp_Y_test_Anes=Y_out_Anes[(Y_ID_Anes == Part_reco[r]) | (Y_ID_Anes == Part_chro[c])]\n",
    "        tmp_Y_train_Anes=Y_out_Anes[(Y_ID_Anes != Part_reco[r]) & (Y_ID_Anes != Part_chro[c])]\n",
    "\n",
    "        tmp_X_test_Reco=X_Reco[(Y_ID_Reco == Part_reco[r]) | (Y_ID_Reco == Part_chro[c])]\n",
    "        tmp_X_train_Reco=X_Reco[(Y_ID_Reco != Part_reco[r]) & (Y_ID_Reco != Part_chro[c])]\n",
    "        tmp_Y_test_Reco=Y_out_Reco[(Y_ID_Reco == Part_reco[r]) | (Y_ID_Reco == Part_chro[c])]\n",
    "        tmp_Y_train_Reco=Y_out_Reco[(Y_ID_Reco != Part_reco[r]) & (Y_ID_Reco != Part_chro[c])]\n",
    "\n",
    "        svm_model = svm.LinearSVC(C=4, loss=\"hinge\", max_iter=100000)\n",
    "        svm_model.fit(tmp_X_train_Base, tmp_Y_train_Base)\n",
    "        P_lr = svm_model.predict(tmp_X_test_Base)\n",
    "        cv_SVM_Base.append(metrics.accuracy_score(tmp_Y_test_Base, P_lr))\n",
    "        FI_SVM_Base.append(svm_model.coef_.flatten())\n",
    "\n",
    "        svm_model = svm.LinearSVC(C=4, loss=\"hinge\", max_iter=100000)\n",
    "        svm_model.fit(tmp_X_train_Anes, tmp_Y_train_Anes)\n",
    "        P_lr = svm_model.predict(tmp_X_test_Anes)\n",
    "        cv_SVM_Anes.append(metrics.accuracy_score(tmp_Y_test_Anes, P_lr))\n",
    "        FI_SVM_Anes.append(svm_model.coef_.flatten())\n",
    "\n",
    "        svm_model = svm.LinearSVC(C=4, loss=\"hinge\", max_iter=100000)\n",
    "        svm_model.fit(tmp_X_train_Reco, tmp_Y_train_Reco)\n",
    "        P_lr = svm_model.predict(tmp_X_test_Reco)\n",
    "        cv_SVM_Reco.append(metrics.accuracy_score(tmp_Y_test_Reco, P_lr))\n",
    "        FI_SVM_Reco.append(svm_model.coef_.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(cv_SVM_Base)\n",
    "plt.plot(cv_SVM_Anes)\n",
    "plt.plot(cv_SVM_Reco)\n",
    "plt.legend(['Baseline','Anesthesia','Recovery'])\n",
    "plt.xlabel('cross validation')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('SVM')\n",
    "plt.savefig('combined_SVM_CV.png',dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(cv_SVM_Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.std(cv_SVM_Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(cv_SVM_Anes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.std(cv_SVM_Anes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(cv_SVM_Reco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.std(cv_SVM_Reco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "right_Anes = np.where(np.array(cv_SVM_Anes) > 0.5)[0]\n",
    "right_Base = np.where(np.array(cv_SVM_Base) > 0.5)[0]\n",
    "right_Reco = np.where(np.array(cv_SVM_Reco) > 0.5)[0]\n",
    "\n",
    "\n",
    "FI_SVM_Base=pd.DataFrame(FI_SVM_Base)\n",
    "FI_SVM_Anes=pd.DataFrame(FI_SVM_Anes)\n",
    "FI_SVM_Reco=pd.DataFrame(FI_SVM_Reco)\n",
    "\n",
    "feat_importances_Base_SVM_b = pd.Series(np.array(abs(np.mean(FI_SVM_Base.iloc[right_Base,:],axis=0))), index=X_Base.columns)\n",
    "feat_importances_Anes_SVM_b = pd.Series(np.array(abs(np.mean(FI_SVM_Anes.iloc[right_Anes,:],axis=0))), index=X_Base.columns)\n",
    "feat_importances_Reco_SVM_b = pd.Series(np.array(abs(np.mean(FI_SVM_Reco.iloc[right_Reco,:],axis=0))), index=X_Base.columns)\n",
    "\n",
    "feat_importances_Base_SVM = pd.Series(np.array((np.mean(FI_SVM_Base.iloc[right_Base,:],axis=0))), index=X_Base.columns)\n",
    "feat_importances_Anes_SVM = pd.Series(np.array((np.mean(FI_SVM_Anes.iloc[right_Anes,:],axis=0))), index=X_Base.columns)\n",
    "feat_importances_Reco_SVM = pd.Series(np.array((np.mean(FI_SVM_Reco.iloc[right_Reco,:],axis=0))), index=X_Base.columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= [10, 6])\n",
    "plt.subplot(121)\n",
    "feat_importances_Base_SVM_b.plot(kind='barh')\n",
    "plt.title('SVM_Baseline')\n",
    "plt.subplot(122)\n",
    "feat_importances_Anes_SVM_b.plot(kind='barh',color='orange')\n",
    "plt.title('Anesthesia')\n",
    "plt.savefig('combined_SVM_features.png',dpi=150)\n",
    "#plt.subplot(133)\n",
    "#feat_importances_Reco_SVM_b.plot(kind='barh',color='green')\n",
    "#plt.title('Recovery')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import graphviz\n",
    "\n",
    "#BAse\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clf = clf.fit(X_Base,Y_out_Base)\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, feature_names=X_Base.columns, class_names=['Chronic', 'recovered'],\n",
    "                                filled=True, rounded=True, special_characters=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render('combined_Decision_Tree_Base')\n",
    "\n",
    "#Anes\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clf = clf.fit(X_Anes,Y_out_Anes)\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, feature_names=X_Base.columns, class_names=['Chronic', 'recovered'],\n",
    "                                filled=True, rounded=True, special_characters=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render('combined_Decision_Tree_Anes')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "data = [[np.max(lr_accuracy_Base), np.max(svm_accuracy_Base)], \n",
    "        [np.max(lr_accuracy_Anes), np.max(svm_accuracy_Anes)],\n",
    "        [np.max(lr_accuracy_Reco), np.max(svm_accuracy_Reco)]]\n",
    "\n",
    "pd.DataFrame(data, columns=[\"test_Logistic Regression\", \"test_SVM\"],index=['Baseline','Anesthesia','Recovery'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "data = [[np.mean(cv_LR_Base), np.mean(cv_SVM_Base)], \n",
    "        [np.mean(cv_LR_Anes), np.mean(cv_SVM_Anes)],\n",
    "        [np.mean(cv_LR_Reco), np.mean(cv_SVM_Reco)]]\n",
    "\n",
    "pd.DataFrame(data, columns=[\"test_Logistic Regression\", \"test_SVM\"],index=['Baseline','Anesthesia','Recovery'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (DOC_Clustering)",
   "language": "python",
   "name": "pycharm-f35155be"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
