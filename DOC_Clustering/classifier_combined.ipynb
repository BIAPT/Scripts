{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import permutation_test_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "from sklearn import metrics\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import dataimport.prepareDataset as prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(141)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "are there features which don't have any further content/ don't change over the whole periode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Part_chro=['13','22','10', '18']\n",
    "Part_reco=['19','20','02','09']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/combined_NEW_wPLI_dPLI_all_10_1_left.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[1;34m(path, encoding)\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mread_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpkl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa: E722\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    145\u001b[0m                             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m                             is_text=False)\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;31m# Python 3 and binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/combined_NEW_wPLI_dPLI_all_10_1_left.pickle'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[1;34m(path, encoding)\u001b[0m\n\u001b[0;32m    172\u001b[0m                 return read_wrapper(\n\u001b[1;32m--> 173\u001b[1;33m                     lambda f: pc.load(f, encoding=encoding, compat=False))\n\u001b[0m\u001b[0;32m    174\u001b[0m             \u001b[1;31m# compat pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    145\u001b[0m                             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m                             is_text=False)\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;31m# Python 3 and binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/combined_NEW_wPLI_dPLI_all_10_1_left.pickle'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(path, compression)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtry_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa: E722\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[1;34m(path, encoding)\u001b[0m\n\u001b[0;32m    176\u001b[0m                 return read_wrapper(\n\u001b[1;32m--> 177\u001b[1;33m                     lambda f: pc.load(f, encoding=encoding, compat=True))\n\u001b[0m\u001b[0;32m    178\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    145\u001b[0m                             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m                             is_text=False)\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;31m# Python 3 and binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/combined_NEW_wPLI_dPLI_all_10_1_left.pickle'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[1;34m(path, encoding)\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mread_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpkl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa: E722\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    145\u001b[0m                             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m                             is_text=False)\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;31m# Python 3 and binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/combined_NEW_wPLI_dPLI_all_10_1_left.pickle'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[1;34m(path, encoding)\u001b[0m\n\u001b[0;32m    172\u001b[0m                 return read_wrapper(\n\u001b[1;32m--> 173\u001b[1;33m                     lambda f: pc.load(f, encoding=encoding, compat=False))\n\u001b[0m\u001b[0;32m    174\u001b[0m             \u001b[1;31m# compat pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    145\u001b[0m                             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m                             is_text=False)\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;31m# Python 3 and binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/combined_NEW_wPLI_dPLI_all_10_1_left.pickle'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-806c12c4329f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m [X_Base,X_Anes,X_Reco,Y_ID_Base,Y_ID_Anes,Y_ID_Reco,\n\u001b[1;32m----> 2\u001b[1;33m  Y_out_Anes,Y_out_Base,Y_out_Reco ]=prep.prepare_Dataset('data/combined_NEW_wPLI_dPLI_all_10_1_left.pickle')\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\Scripts\\DOC_Clustering\\dataimport\\prepareDataset.py\u001b[0m in \u001b[0;36mprepare_Dataset\u001b[1;34m(datafile)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mnames_combined\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnames_combined\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'd_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mareas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatafile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mY_ID\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(path, compression)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa: E722\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mPY3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtry_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[1;34m(path, encoding)\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa: E722\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m                 return read_wrapper(\n\u001b[1;32m--> 177\u001b[1;33m                     lambda f: pc.load(f, encoding=encoding, compat=True))\n\u001b[0m\u001b[0;32m    178\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtry_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    144\u001b[0m         f, fh = _get_handle(path, 'rb',\n\u001b[0;32m    145\u001b[0m                             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m                             is_text=False)\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    428\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;31m# Python 3 and binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/combined_NEW_wPLI_dPLI_all_10_1_left.pickle'"
     ]
    }
   ],
   "source": [
    "\n",
    "[X_Base,X_Anes,X_Reco,Y_ID_Base,Y_ID_Anes,Y_ID_Reco,\n",
    " Y_out_Anes,Y_out_Base,Y_out_Reco ]=prep.prepare_Dataset('data/combined_NEW_wPLI_dPLI_all_10_1_left.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define 30% test set \n",
    "!!!! only used to select the hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_Base)\n",
    "X_Base=scaler.transform(X_Base)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_Anes)\n",
    "X_Anes=scaler.transform(X_Anes)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_Reco)\n",
    "X_Reco=scaler.transform(X_Reco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#X_train,X_test,Y_train,Y_test=train_test_split(X,Y_out,Y_ID,random_state=0,test_size=0.3)\n",
    "X_train_Base,X_test_Base,Y_train_Base,Y_test_Base,Y_ID_Base_train,Y_ID_Base_test=train_test_split(X_Base,Y_out_Base,Y_ID_Base,random_state=0,test_size=0.3)\n",
    "X_train_Reco,X_test_Reco,Y_train_Reco,Y_test_Reco,Y_ID_Reco_train,Y_ID_Reco_test=train_test_split(X_Reco,Y_out_Reco,Y_ID_Reco,random_state=0,test_size=0.3)\n",
    "X_train_Anes,X_test_Anes,Y_train_Anes,Y_test_Anes,Y_ID_Anes_train,Y_ID_Anes_test=train_test_split(X_Anes,Y_out_Anes,Y_ID_Anes,random_state=0,test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr=LogisticRegression(random_state=0,penalty='l1',C=4,solver='liblinear',max_iter=10000)\n",
    "\n",
    "#score_B, permutation_scores_B, pvalue_B = permutation_test_score(\n",
    "#    lr, X_Base,Y_out_Base, scoring=\"accuracy\", n_permutations=100, n_jobs=-1,cv=3)\n",
    "\n",
    "#score_A, permutation_scores_A, pvalue_A = permutation_test_score(\n",
    "#    lr, X_Anes,Y_out_Anes, scoring=\"accuracy\", n_permutations=100, n_jobs=-1,cv=3)\n",
    "\n",
    "#score_R, permutation_scores_R, pvalue_R = permutation_test_score(\n",
    "#    lr, X_Reco,Y_out_Reco, scoring=\"accuracy\", n_permutations=100, n_jobs=-1,cv=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View histogram of permutation scores\n",
    "\"\"\"\n",
    "plt.figure(figsize= [15, 6])\n",
    "plt.subplot(131)\n",
    "plt.hist(permutation_scores_B, 20, label='Permutation scores Baseline')\n",
    "ylim = plt.ylim()\n",
    "plt.plot(2 * [score_B], ylim, '--g', linewidth=3,\n",
    "         label='Classification Score'\n",
    "         ' (pvalue %s)' % pvalue_B)\n",
    "plt.plot(2 * [1. / 2], ylim, '--k', linewidth=3, label='Luck')\n",
    "plt.legend()\n",
    "plt.xlabel('Score')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.hist(permutation_scores_A, 20, label='Permutation scores Anesthesia')\n",
    "ylim = plt.ylim()\n",
    "plt.plot(2 * [score_A], ylim, '--g', linewidth=3,\n",
    "         label='Classification Score'\n",
    "         ' (pvalue %s)' % pvalue_A)\n",
    "plt.plot(2 * [1. / 2], ylim, '--k', linewidth=3, label='Luck')\n",
    "plt.legend()\n",
    "plt.xlabel('Score')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.hist(permutation_scores_R, 20, label='Permutation scores Recovery')\n",
    "ylim = plt.ylim()\n",
    "plt.plot(2 * [score_R], ylim, '--g', linewidth=3,\n",
    "         label='Classification Score'\n",
    "         ' (pvalue %s)' % pvalue_R)\n",
    "plt.plot(2 * [1. / 2], ylim, '--k', linewidth=3, label='Luck')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Score')\n",
    "plt.savefig('combined_LogReg_permutation.png',dpi=150)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data = [[score_B, pvalue_B], \n",
    "        [score_A, pvalue_A],\n",
    "        [score_R, pvalue_R]]\n",
    "\n",
    "pd.DataFrame(data, columns=[\"permutation LR\", \"pvalue\"],index=['Baseline','Anesthesia','Recovery'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "LOGISTIC REGRESSION\n",
    "  I Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cs=np.arange(1,4,0.5)\n",
    "lr_accuracy_Base=[]\n",
    "lr_accuracy_Anes=[]\n",
    "lr_accuracy_Reco=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for c in cs:\n",
    "    lr=LogisticRegression(random_state=0,penalty='l1',C=c,solver='liblinear',max_iter=10000)\n",
    "    lr.fit(X_train_Base,Y_train_Base)\n",
    "    P_lr=lr.predict(X_test_Base)\n",
    "    lr_accuracy_Base.append(metrics.accuracy_score(Y_test_Base, P_lr))\n",
    "\n",
    "    lr=LogisticRegression(random_state=0,penalty='l1',C=c,solver='liblinear',max_iter=10000)\n",
    "    lr.fit(X_train_Anes,Y_train_Anes)\n",
    "    P_lr=lr.predict(X_test_Anes)\n",
    "    lr_accuracy_Anes.append(metrics.accuracy_score(Y_test_Anes, P_lr))\n",
    "\n",
    "    lr=LogisticRegression(random_state=0,penalty='l1',C=c,solver='liblinear',max_iter=10000)\n",
    "    lr.fit(X_train_Reco,Y_train_Reco)\n",
    "    P_lr=lr.predict(X_test_Reco)\n",
    "    lr_accuracy_Reco.append(metrics.accuracy_score(Y_test_Reco, P_lr))\n",
    "\n",
    "#feat_importances_Reco = pd.Series(lr.coef_[0], index=X.columns)\n",
    "#feat_importances_Reco.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(cs,lr_accuracy_Base)\n",
    "plt.plot(cs,lr_accuracy_Anes)\n",
    "plt.plot(cs,lr_accuracy_Reco)\n",
    "plt.ylabel('Accuracy [%]')\n",
    "plt.xlabel('C')\n",
    "plt.title('Linear Regression')\n",
    "plt.legend(['Baseline','Anesthesia','Recovery'])\n",
    "plt.savefig('combined_LogReg_hyper.png',dpi=150)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr=LogisticRegression(random_state=0,penalty='l1',C=c,solver='liblinear',max_iter=10000)\n",
    "lr.fit(X_train_Anes,Y_train_Anes)\n",
    "P_lr=lr.predict(X_test_Anes)\n",
    "lr_accuracy_Anes.append(metrics.accuracy_score(Y_test_Anes, P_lr))\n",
    "\n",
    "lr=LogisticRegression(random_state=0,penalty='l1',C=c,solver='liblinear',max_iter=10000)\n",
    "lr.fit(X_train_Reco,Y_train_Reco)\n",
    "P_lr=lr.predict(X_test_Reco)\n",
    "lr_accuracy_Reco.append(metrics.accuracy_score(Y_test_Reco, P_lr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "cv_LR_Base=[]\n",
    "cv_LR_Anes=[]\n",
    "cv_LR_Reco=[]\n",
    "\n",
    "FI_LR_Base=[]\n",
    "FI_LR_Anes=[]\n",
    "FI_LR_Reco=[]\n",
    "\n",
    "for r in range(0,4):\n",
    "    for c in range (0,4):\n",
    "        tmp_X_test_Base=X_Base[(Y_ID_Base == Part_reco[r]) | (Y_ID_Base == Part_chro[c])]\n",
    "        tmp_X_train_Base=X_Base[(Y_ID_Base != Part_reco[r]) & (Y_ID_Base != Part_chro[c])]\n",
    "        tmp_Y_test_Base=Y_out_Base[(Y_ID_Base == Part_reco[r]) | (Y_ID_Base == Part_chro[c])]\n",
    "        tmp_Y_train_Base=Y_out_Base[(Y_ID_Base != Part_reco[r]) & (Y_ID_Base != Part_chro[c])]\n",
    "        \n",
    "        tmp_X_test_Anes=X_Anes[(Y_ID_Anes == Part_reco[r]) | (Y_ID_Anes == Part_chro[c])]\n",
    "        tmp_X_train_Anes=X_Anes[(Y_ID_Anes != Part_reco[r]) & (Y_ID_Anes != Part_chro[c])]\n",
    "        tmp_Y_test_Anes=Y_out_Anes[(Y_ID_Anes == Part_reco[r]) | (Y_ID_Anes == Part_chro[c])]\n",
    "        tmp_Y_train_Anes=Y_out_Anes[(Y_ID_Anes != Part_reco[r]) & (Y_ID_Anes != Part_chro[c])]\n",
    "\n",
    "        tmp_X_test_Reco=X_Reco[(Y_ID_Reco == Part_reco[r]) | (Y_ID_Reco == Part_chro[c])]\n",
    "        tmp_X_train_Reco=X_Reco[(Y_ID_Reco != Part_reco[r]) & (Y_ID_Reco != Part_chro[c])]\n",
    "        tmp_Y_test_Reco=Y_out_Reco[(Y_ID_Reco == Part_reco[r]) | (Y_ID_Reco == Part_chro[c])]\n",
    "        tmp_Y_train_Reco=Y_out_Reco[(Y_ID_Reco != Part_reco[r]) & (Y_ID_Reco != Part_chro[c])]\n",
    "\n",
    "        lr = LogisticRegression(random_state=0, penalty='l1', C=4,max_iter=1000,solver='liblinear')\n",
    "        lr.fit(tmp_X_train_Base, tmp_Y_train_Base)\n",
    "        P_lr = lr.predict(tmp_X_test_Base)\n",
    "        cv_LR_Base.append(metrics.accuracy_score(tmp_Y_test_Base, P_lr))\n",
    "        FI_LR_Base.append(lr.coef_)\n",
    "        \n",
    "\n",
    "        lr = LogisticRegression(random_state=0, penalty='l1', C=4,max_iter=1000,solver='liblinear')\n",
    "        lr.fit(tmp_X_train_Anes, tmp_Y_train_Anes)\n",
    "        P_lr = lr.predict(tmp_X_test_Anes)\n",
    "        cv_LR_Anes.append(metrics.accuracy_score(tmp_Y_test_Anes, P_lr))\n",
    "        FI_LR_Anes.append(lr.coef_)\n",
    "        \n",
    "        lr = LogisticRegression(random_state=0, penalty='l1', C=4,max_iter=1000,solver='liblinear')\n",
    "        lr.fit(tmp_X_train_Reco, tmp_Y_train_Reco)\n",
    "        P_lr = lr.predict(tmp_X_test_Reco)\n",
    "        cv_LR_Reco.append(metrics.accuracy_score(tmp_Y_test_Reco, P_lr))\n",
    "        FI_LR_Reco.append(lr.coef_)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(cv_LR_Base)\n",
    "plt.plot(cv_LR_Anes)\n",
    "plt.plot(cv_LR_Reco)\n",
    "plt.legend(['Baseline','Anesthesia','Recovery'])\n",
    "plt.xlabel('cross validation')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('combined_CV')\n",
    "plt.savefig('combined_LogRefg_CV.png',dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "right_Anes = np.where(np.array(cv_LR_Anes) > 0.5)[0]\n",
    "right_Base = np.where(np.array(cv_LR_Base) > 0.5)[0]\n",
    "right_Reco = np.where(np.array(cv_LR_Reco) > 0.5)[0]\n",
    "\n",
    "FI_LR_Base= list(FI_LR_Base[i] for i in right_Base)\n",
    "FI_LR_Anes= list(FI_LR_Anes[i] for i in right_Anes)\n",
    "FI_LR_Reco= list(FI_LR_Reco[i] for i in right_Reco)\n",
    "\n",
    "feat_importances_Base_LR = pd.Series(abs(np.mean(FI_LR_Base[0:],axis=0)[0]), index=X_Base.columns)\n",
    "feat_importances_Anes_LR = pd.Series(abs(np.mean(FI_LR_Anes[0:],axis=0)[0]), index=X_Base.columns)\n",
    "feat_importances_Reco_LR = pd.Series(abs(np.mean(FI_LR_Reco[0:],axis=0)[0]), index=X_Base.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= [10, 6])\n",
    "plt.subplot(121)\n",
    "feat_importances_Base_LR.plot(kind='barh')\n",
    "plt.title('LR_Baseline')\n",
    "plt.subplot(122)\n",
    "feat_importances_Anes_LR.plot(kind='barh',color='orange')\n",
    "plt.title('Anesthesia')\n",
    "plt.savefig('combined_LogRefg_features.png',dpi=150)\n",
    "#plt.subplot(133)\n",
    "#feat_importances_Reco_LR.plot(kind='barh',color='green')\n",
    "#plt.title('Recovery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(cv_LR_Base)\n",
    "np.std(cv_LR_Base)\n",
    "\n",
    "np.mean(cv_LR_Anes)\n",
    "np.std(cv_LR_Anes)\n",
    "\n",
    "np.mean(cv_LR_Reco)\n",
    "np.std(cv_LR_Reco)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "SVM (sklearn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cs=np.arange(0.3,6,0.2)\n",
    "\n",
    "svm_accuracy_Base=[]\n",
    "svm_accuracy_Anes=[]\n",
    "svm_accuracy_Reco=[]\n",
    "\n",
    "for c in cs:\n",
    "    svm_model = svm.LinearSVC(C=c, loss=\"hinge\" , max_iter=100000)\n",
    "    svm_model.fit(X_train_Base,Y_train_Base)\n",
    "    P_lr=svm_model.predict(X_test_Base)\n",
    "    svm_accuracy_Base.append(metrics.accuracy_score(Y_test_Base, P_lr))\n",
    "\n",
    "    svm_model = svm.LinearSVC(C=c, loss=\"hinge\", max_iter=100000)\n",
    "    svm_model.fit(X_train_Anes,Y_train_Anes)\n",
    "    P_lr=svm_model.predict(X_test_Anes)\n",
    "    svm_accuracy_Anes.append(metrics.accuracy_score(Y_test_Anes, P_lr))\n",
    "\n",
    "    svm_model = svm.LinearSVC(C=c, loss=\"hinge\", max_iter=100000)\n",
    "    svm_model.fit(X_train_Reco,Y_train_Reco)\n",
    "    P_lr=svm_model.predict(X_test_Reco)\n",
    "    svm_accuracy_Reco.append(metrics.accuracy_score(Y_test_Reco, P_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(cs,svm_accuracy_Base)\n",
    "plt.plot(cs,svm_accuracy_Anes)\n",
    "plt.plot(cs,svm_accuracy_Reco)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('C')\n",
    "plt.title('Support Vector Machine')\n",
    "plt.legend(['Baseline','Anesthesia','Recovery'])\n",
    "plt.show()\n",
    "plt.savefig('combined_SVM_hyper.png',dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cv_SVM_Base=[]\n",
    "cv_SVM_Anes=[]\n",
    "cv_SVM_Reco=[]\n",
    "\n",
    "FI_SVM_Base=[]\n",
    "FI_SVM_Anes=[]\n",
    "FI_SVM_Reco=[]\n",
    "\n",
    "\n",
    "for r in range(0,4):\n",
    "    for c in range (0,4):\n",
    "        tmp_X_test_Base=X_Base[(Y_ID_Base == Part_reco[r]) | (Y_ID_Base == Part_chro[c])]\n",
    "        tmp_X_train_Base=X_Base[(Y_ID_Base != Part_reco[r]) & (Y_ID_Base != Part_chro[c])]\n",
    "        tmp_Y_test_Base=Y_out_Base[(Y_ID_Base == Part_reco[r]) | (Y_ID_Base == Part_chro[c])]\n",
    "        tmp_Y_train_Base=Y_out_Base[(Y_ID_Base != Part_reco[r]) & (Y_ID_Base != Part_chro[c])]\n",
    "\n",
    "        tmp_X_test_Anes=X_Anes[(Y_ID_Anes == Part_reco[r]) | (Y_ID_Anes == Part_chro[c])]\n",
    "        tmp_X_train_Anes=X_Anes[(Y_ID_Anes != Part_reco[r]) & (Y_ID_Anes != Part_chro[c])]\n",
    "        tmp_Y_test_Anes=Y_out_Anes[(Y_ID_Anes == Part_reco[r]) | (Y_ID_Anes == Part_chro[c])]\n",
    "        tmp_Y_train_Anes=Y_out_Anes[(Y_ID_Anes != Part_reco[r]) & (Y_ID_Anes != Part_chro[c])]\n",
    "\n",
    "        tmp_X_test_Reco=X_Reco[(Y_ID_Reco == Part_reco[r]) | (Y_ID_Reco == Part_chro[c])]\n",
    "        tmp_X_train_Reco=X_Reco[(Y_ID_Reco != Part_reco[r]) & (Y_ID_Reco != Part_chro[c])]\n",
    "        tmp_Y_test_Reco=Y_out_Reco[(Y_ID_Reco == Part_reco[r]) | (Y_ID_Reco == Part_chro[c])]\n",
    "        tmp_Y_train_Reco=Y_out_Reco[(Y_ID_Reco != Part_reco[r]) & (Y_ID_Reco != Part_chro[c])]\n",
    "\n",
    "        svm_model = svm.LinearSVC(C=4, loss=\"hinge\", max_iter=100000)\n",
    "        svm_model.fit(tmp_X_train_Base, tmp_Y_train_Base)\n",
    "        P_lr = svm_model.predict(tmp_X_test_Base)\n",
    "        cv_SVM_Base.append(metrics.accuracy_score(tmp_Y_test_Base, P_lr))\n",
    "        FI_SVM_Base.append(svm_model.coef_.flatten())\n",
    "\n",
    "        svm_model = svm.LinearSVC(C=4, loss=\"hinge\", max_iter=100000)\n",
    "        svm_model.fit(tmp_X_train_Anes, tmp_Y_train_Anes)\n",
    "        P_lr = svm_model.predict(tmp_X_test_Anes)\n",
    "        cv_SVM_Anes.append(metrics.accuracy_score(tmp_Y_test_Anes, P_lr))\n",
    "        FI_SVM_Anes.append(svm_model.coef_.flatten())\n",
    "\n",
    "        svm_model = svm.LinearSVC(C=4, loss=\"hinge\", max_iter=100000)\n",
    "        svm_model.fit(tmp_X_train_Reco, tmp_Y_train_Reco)\n",
    "        P_lr = svm_model.predict(tmp_X_test_Reco)\n",
    "        cv_SVM_Reco.append(metrics.accuracy_score(tmp_Y_test_Reco, P_lr))\n",
    "        FI_SVM_Reco.append(svm_model.coef_.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(cv_SVM_Base)\n",
    "plt.plot(cv_SVM_Anes)\n",
    "plt.plot(cv_SVM_Reco)\n",
    "plt.legend(['Baseline','Anesthesia','Recovery'])\n",
    "plt.xlabel('cross validation')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('SVM')\n",
    "plt.savefig('combined_SVM_CV.png',dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(cv_SVM_Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.std(cv_SVM_Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(cv_SVM_Anes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.std(cv_SVM_Anes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(cv_SVM_Reco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.std(cv_SVM_Reco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "right_Anes = np.where(np.array(cv_SVM_Anes) > 0.5)[0]\n",
    "right_Base = np.where(np.array(cv_SVM_Base) > 0.5)[0]\n",
    "right_Reco = np.where(np.array(cv_SVM_Reco) > 0.5)[0]\n",
    "\n",
    "\n",
    "FI_SVM_Base=pd.DataFrame(FI_SVM_Base)\n",
    "FI_SVM_Anes=pd.DataFrame(FI_SVM_Anes)\n",
    "FI_SVM_Reco=pd.DataFrame(FI_SVM_Reco)\n",
    "\n",
    "feat_importances_Base_SVM_b = pd.Series(np.array(abs(np.mean(FI_SVM_Base.iloc[right_Base,:],axis=0))), index=X_Base.columns)\n",
    "feat_importances_Anes_SVM_b = pd.Series(np.array(abs(np.mean(FI_SVM_Anes.iloc[right_Anes,:],axis=0))), index=X_Base.columns)\n",
    "feat_importances_Reco_SVM_b = pd.Series(np.array(abs(np.mean(FI_SVM_Reco.iloc[right_Reco,:],axis=0))), index=X_Base.columns)\n",
    "\n",
    "feat_importances_Base_SVM = pd.Series(np.array((np.mean(FI_SVM_Base.iloc[right_Base,:],axis=0))), index=X_Base.columns)\n",
    "feat_importances_Anes_SVM = pd.Series(np.array((np.mean(FI_SVM_Anes.iloc[right_Anes,:],axis=0))), index=X_Base.columns)\n",
    "feat_importances_Reco_SVM = pd.Series(np.array((np.mean(FI_SVM_Reco.iloc[right_Reco,:],axis=0))), index=X_Base.columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= [10, 6])\n",
    "plt.subplot(121)\n",
    "feat_importances_Base_SVM_b.plot(kind='barh')\n",
    "plt.title('SVM_Baseline')\n",
    "plt.subplot(122)\n",
    "feat_importances_Anes_SVM_b.plot(kind='barh',color='orange')\n",
    "plt.title('Anesthesia')\n",
    "plt.savefig('combined_SVM_features.png',dpi=150)\n",
    "#plt.subplot(133)\n",
    "#feat_importances_Reco_SVM_b.plot(kind='barh',color='green')\n",
    "#plt.title('Recovery')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import graphviz\n",
    "\n",
    "#BAse\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clf = clf.fit(X_Base,Y_out_Base)\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, feature_names=X_Base.columns, class_names=['Chronic', 'recovered'],\n",
    "                                filled=True, rounded=True, special_characters=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render('combined_Decision_Tree_Base')\n",
    "\n",
    "#Anes\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clf = clf.fit(X_Anes,Y_out_Anes)\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, feature_names=X_Base.columns, class_names=['Chronic', 'recovered'],\n",
    "                                filled=True, rounded=True, special_characters=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render('combined_Decision_Tree_Anes')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "data = [[np.max(lr_accuracy_Base), np.max(svm_accuracy_Base)], \n",
    "        [np.max(lr_accuracy_Anes), np.max(svm_accuracy_Anes)],\n",
    "        [np.max(lr_accuracy_Reco), np.max(svm_accuracy_Reco)]]\n",
    "\n",
    "pd.DataFrame(data, columns=[\"test_Logistic Regression\", \"test_SVM\"],index=['Baseline','Anesthesia','Recovery'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "data = [[np.mean(cv_LR_Base), np.mean(cv_SVM_Base)], \n",
    "        [np.mean(cv_LR_Anes), np.mean(cv_SVM_Anes)],\n",
    "        [np.mean(cv_LR_Reco), np.mean(cv_SVM_Reco)]]\n",
    "\n",
    "pd.DataFrame(data, columns=[\"test_Logistic Regression\", \"test_SVM\"],index=['Baseline','Anesthesia','Recovery'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
